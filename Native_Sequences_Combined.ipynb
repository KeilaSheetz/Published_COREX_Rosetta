{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SCORE NATIVE SEQUENCES ON IMPORTED STRUCTURES \n",
    "\n",
    "This script is for scoring experimentally solved structures with both COREX and Rosetta.\n",
    "\n",
    "The only input for this script are a list of PDB codes. This import is made at the beginning of the script \n",
    "directly after the import cell - this is the only cell you are required to edit.\n",
    "\n",
    "The output file will contain all the structures scored by Rosetta (in REU) \n",
    "and COREX (in Total LogOdds, also referred to as the Fold Recognition Score)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyRosetta-4 2020 [Rosetta PyRosetta4.Release.python37.mac 2020.20+release.c522e9e9054813e62f9415bcc29478af67fee549 2020-05-14T10:55:54] retrieved from: http://www.pyrosetta.org\n",
      "(C) Copyright Rosetta Commons Member Institutions. Created in JHU by Sergey Lyskov and PyRosetta Team.\n",
      "\u001b[0mcore.init: \u001b[0mChecking for fconfig files in pwd and ./rosetta/flags\n",
      "\u001b[0mcore.init: \u001b[0mRosetta version: PyRosetta4.Release.python37.mac r255 2020.20+release.c522e9e9054 c522e9e9054813e62f9415bcc29478af67fee549 http://www.pyrosetta.org 2020-05-14T10:55:54\n",
      "\u001b[0mcore.init: \u001b[0mcommand: PyRosetta -relax:constrain_relax_to_start_coords -use_input_sc -relax:ramp_constraints false -ex1 -ex2 -packing:flip_HNQ -no_optH -no_optH false -database /Users/keilasheetz/opt/anaconda3/lib/python3.7/site-packages/pyrosetta-2020.20+release.c522e9e9054-py3.7-macosx-10.9-x86_64.egg/pyrosetta/database\n",
      "\u001b[0mbasic.random.init_random_generator: \u001b[0m'RNG device' seed mode, using '/dev/urandom', seed=1979938160 seed_offset=0 real_seed=1979938160\n",
      "\u001b[0mbasic.random.init_random_generator: \u001b[0mRandomGenerator:init: Normal mode, seed=1979938160 RG_type=mt19937\n",
      "\u001b[0mcore.scoring.ScoreFunctionFactory: \u001b[0mSCOREFUNCTION: \u001b[32mref2015\u001b[0m\n",
      "\u001b[0mcore.scoring.etable: \u001b[0mStarting energy table calculation\n",
      "\u001b[0mcore.scoring.etable: \u001b[0msmooth_etable: changing atr/rep split to bottom of energy well\n",
      "\u001b[0mcore.scoring.etable: \u001b[0msmooth_etable: spline smoothing lj etables (maxdis = 6)\n",
      "\u001b[0mcore.scoring.etable: \u001b[0msmooth_etable: spline smoothing solvation etables (max_dis = 6)\n",
      "\u001b[0mcore.scoring.etable: \u001b[0mFinished calculating energy tables.\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/hbonds/ref2015_params/HBPoly1D.csv\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/hbonds/ref2015_params/HBFadeIntervals.csv\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/hbonds/ref2015_params/HBEval.csv\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/hbonds/ref2015_params/DonStrength.csv\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/hbonds/ref2015_params/AccStrength.csv\n",
      "\u001b[0mcore.chemical.GlobalResidueTypeSet: \u001b[0mFinished initializing fa_standard residue type set.  Created 980 residue types\n",
      "\u001b[0mcore.chemical.GlobalResidueTypeSet: \u001b[0mTotal time to initialize 1.03416 seconds.\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/rama/fd/all.ramaProb\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/rama/fd/prepro.ramaProb\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/omega/omega_ppdep.all.txt\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/omega/omega_ppdep.gly.txt\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/omega/omega_ppdep.pro.txt\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/omega/omega_ppdep.valile.txt\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/P_AA_pp/P_AA\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/P_AA_pp/P_AA_n\n",
      "\u001b[0mcore.scoring.P_AA: \u001b[0mshapovalov_lib::shap_p_aa_pp_smooth_level of 1( aka low_smooth ) got activated.\n",
      "\u001b[0mbasic.io.database: \u001b[0mDatabase file opened: scoring/score_functions/P_AA_pp/shapovalov/10deg/kappa131/a20.prop\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mReading relax scripts list from database.\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mLooking for MonomerRelax2019.txt\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0m================== Reading script file: /Users/keilasheetz/opt/anaconda3/lib/python3.7/site-packages/pyrosetta-2020.20+release.c522e9e9054-py3.7-macosx-10.9-x86_64.egg/pyrosetta/database/sampling/relax_scripts/MonomerRelax2019.txt ==================\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mrepeat %%nrepeats%%\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mcoord_cst_weight 1.0\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mscale:fa_rep 0.040\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mrepack\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mscale:fa_rep 0.051\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mmin 0.01\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mcoord_cst_weight 0.5\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mscale:fa_rep 0.265\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mrepack\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mscale:fa_rep 0.280\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mmin 0.01\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mcoord_cst_weight 0.0\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mscale:fa_rep 0.559\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mrepack\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mscale:fa_rep 0.581\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mmin 0.01\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mcoord_cst_weight 0.0\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mscale:fa_rep 1\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mrepack\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mmin 0.00001\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0maccept_to_best\n",
      "\u001b[0mprotocols.relax.RelaxScriptManager: \u001b[0mendrepeat\n",
      "Today is 231107\n",
      "The time is 14:26:55\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IMPORTS\n",
    "\"\"\"\n",
    "\n",
    "import Bio\n",
    "from Bio.PDB import *\n",
    "from biopandas.pdb import PandasPdb\n",
    "from Bio.PDB import PDBList\n",
    "import csv\n",
    "from datetime import date, datetime\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import math      \n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np; np.random.seed(1)\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from subprocess import call, PIPE, Popen\n",
    "from subprocess import PIPE, STDOUT\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import norm\n",
    "import shutil\n",
    "import statistics\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "# Imports and initiates PyRosetta\n",
    "from pyrosetta import *\n",
    "from pyrosetta.rosetta import *\n",
    "from pyrosetta.teaching import *\n",
    "init(\"-relax:constrain_relax_to_start_coords \"\n",
    "     \"-use_input_sc \"\n",
    "     \"-relax:ramp_constraints false \"\n",
    "     \"-ex1 \"\n",
    "     \"-ex2 \"\n",
    "     \"-packing:flip_HNQ \"\n",
    "     \"-no_optH \"\n",
    "     \"-no_optH false\")\n",
    "# Init a score function from rosetta\n",
    "sf = pyrosetta.get_fa_scorefxn()\n",
    "# Init fast relax from rosetta protocols\n",
    "fr = pyrosetta.rosetta.protocols.relax.FastRelax(sf)\n",
    "\n",
    "# Record today's date for file generation. Beware! If generating multiple files on the\n",
    "# same day, this will overwrite unless you intervene \n",
    "today = date.today()\n",
    "date = today.strftime(\"%y%m%d\")\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "print(\"Today is\", date)\n",
    "print(\"The time is\", current_time)\n",
    "\n",
    "# Establish the current directory \n",
    "i_am_here = os.getcwd()\n",
    "\n",
    "# Create internal directories for output files if they aren't already present\n",
    "# Name paths for these directories to better organize output files \n",
    "\n",
    "# Find the PDB directory if it exists, otherwise create one\n",
    "if not os.path.exists('%s/PDBs'%i_am_here):\n",
    "    os.makedirs('%s/PDBs'%i_am_here)\n",
    "pdb_dir = '%s/PDBs'%i_am_here\n",
    "\n",
    "# Find the Rosetta directory if it exists, otherwise create one\n",
    "if not os.path.exists('%s/Rosetta'%i_am_here):\n",
    "    os.makedirs('%s/Rosetta'%i_am_here)\n",
    "ros_dir = '%s/Rosetta'%i_am_here\n",
    "\n",
    "# Find the COREX directory if it exists, otherwise create one\n",
    "if not os.path.exists('%s/COREX_Output'%i_am_here):\n",
    "    os.makedirs('%s/COREX_Output'%i_am_here)\n",
    "cor_dir = '%s/COREX_Output'%i_am_here\n",
    "corex_files = '%s/COREX_Files'%i_am_here\n",
    "\n",
    "native_logodds = pd.read_csv('%s/Native_Log_Odds.csv'%corex_files)\n",
    "denat_logodds = pd.read_csv('%s/Denatured_Log_Odds.csv'%corex_files)\n",
    "\n",
    "aa_dict = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "     'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N', \n",
    "     'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W', \n",
    "     'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2dyrF', '2dyrG', '4ubpA', '5fhcJ', '5hmgB']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ESTABLISH ALL IMPORT FILES\n",
    "\"\"\"\n",
    "# Establish the path to the files containing the PDB structures to import \n",
    "# Establish the path to the sequences to thread on to the PDB structures and a unique identifier\n",
    "\n",
    "# CSV file of PDB codes\n",
    "pdb_csv = 'your_pdbs.csv'\n",
    "\n",
    "pdb_list = []\n",
    "with open(pdb_csv, 'r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        pdb_list.append(row)\n",
    "        \n",
    "pdb_list = list(chain.from_iterable(pdb_list))\n",
    "print(pdb_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2dyrF', '2dyrG', '4ubpA', '5fhcJ', '5hmgB']\n",
      "2dyr\n",
      "Downloading PDB structure '2dyr'...\n",
      "2dyr\n",
      "Downloading PDB structure '2dyr'...\n",
      "4ubp\n",
      "Downloading PDB structure '4ubp'...\n",
      "5fhc\n",
      "Downloading PDB structure '5fhc'...\n",
      "5hmg\n",
      "Downloading PDB structure '5hmg'...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IMPORT THE LIST OF PDBs AND DOWNLOAD THEM \n",
    "\"\"\"\n",
    "# This script will download the PDB files necessary if they are not already in the current directory\n",
    "# You can skip this step if you have already downloaded the pdb files \n",
    "\n",
    "print(pdb_list)\n",
    "errors = []\n",
    "\n",
    "ppdb0 = []\n",
    "ppdb1 = []\n",
    "pdbl = PDBList()\n",
    "\n",
    "for i in pdb_list:\n",
    "    try:\n",
    "        code = i[0:4]\n",
    "        print(code)\n",
    "        ppdb0.append(pdbl.retrieve_pdb_file(code,pdir='.', file_format ='pdb'))\n",
    "        ppdb1.append(PandasPdb().fetch_pdb(code))\n",
    "        \n",
    "        shutil.move(i_am_here + '/pdb%s.ent'%code.lower(), pdb_dir + '/pdb%s.ent'%code.lower())\n",
    "   \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print('%s was not downloaded'%i)\n",
    "        errors.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2dyr\n",
      "F\n",
      "2dyr\n",
      "G\n",
      "4ubp\n",
      "A\n",
      "5fhc\n",
      "J\n",
      "5hmg\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CLEAN ALL FILES\n",
    "\"\"\"\n",
    "# Remove any hydrogens so that COREX can handle all PDB files\n",
    "# Right now this can handle NMR PDBs, but will ONLY take model 1\n",
    "# If you would like to select a different model, it should be manually done before running this cell\n",
    "\n",
    "# Iterate through the directory and find all PDBs that have not been processed\n",
    "for pdb in pdb_list:\n",
    "    \n",
    "    for file in os.listdir(pdb_dir):\n",
    "\n",
    "        if file.endswith(\".ent\") and pdb[0:4].lower() or pdb[0:4] in file:\n",
    "\n",
    "            pdb_name = file[3:7]\n",
    "            print(pdb_name)\n",
    "            \n",
    "            if len(pdb)==5:\n",
    "                chain = pdb[4]\n",
    "                print(chain)\n",
    "            else:\n",
    "                chain = 'A'\n",
    "\n",
    "            if \"%s.cleaned.pdb\" %pdb not in os.listdir(pdb_dir):\n",
    "\n",
    "                # Open the file and only keep non-H atoms\n",
    "                # If there are multiple models, only keep model 1 atoms\n",
    "\n",
    "                name = (os.path.join('%s/PDBs/'%i_am_here, file))\n",
    "                fo = open(name, \"r\")\n",
    "                lines = fo.readlines()\n",
    "\n",
    "                cleanlist = []\n",
    "\n",
    "                for line in lines:\n",
    "                    if line[0:4] == \"ATOM\" and line[21:22]==chain:\n",
    "                        if not line[77:78] == 'H':\n",
    "                            if line[16:17] == ' ' or line[16:17] == 'A':\n",
    "                                cleanlist.append(line)\n",
    "\n",
    "                    elif line[0:6] == \"ENDMDL\":\n",
    "                        break\n",
    "\n",
    "                cleanlist.append('END')\n",
    "\n",
    "                with open('%s.cleaned.pdb' %pdb, 'w') as filehandle:\n",
    "                    for item in cleanlist:\n",
    "                        filehandle.write('%s' %item)\n",
    "\n",
    "                fo.close()\n",
    "\n",
    "                shutil.move('%s/%s.cleaned.pdb'%(i_am_here, pdb), \n",
    "                            '%s/%s.cleaned.pdb'%(pdb_dir, pdb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATE THE NATIVE POSES WITH ROSETTA\n",
    "\"\"\"\n",
    "# Score all PDBs with their native structure\n",
    "# The output file contains all of the PDB scores, sequences, and runtimes\n",
    "\n",
    "scores = []\n",
    "\n",
    "for pdb in pdb_list:\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    pose = pyrosetta.pose_from_pdb(\"%s/%s.cleaned.pdb\"%(pdb_dir, pdb))\n",
    "    seq = pose.sequence()\n",
    "    \n",
    "    # Minimize the pose using fast relax and the full atom score function\n",
    "    fr.apply(pose)\n",
    "    # Score the post (ensures score data is included in the PDB file dump)\n",
    "    score = sf(pose)\n",
    "\n",
    "    pose.dump_pdb('%s/%s.rosetta.pdb'%(ros_dir, pdb))\n",
    "\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "\n",
    "    scores.append([pdb, seq, score, runtime])\n",
    "\n",
    "    df = pd.DataFrame(mutant_scores, columns = ['PDB', 'Sequence', 'Rosetta Score', 'Runtime'])\n",
    "    df.to_csv('%s_rosetta_data.csv'%date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ENSEMBLE GENERATION\n",
    "\"\"\"\n",
    "# Create ensemble (.5.4) files for all proteins\n",
    "# If the ensemble file is already in the directory, it will skip that PDB \n",
    "\n",
    "# This outputs the progress, useful if multiple PDBs are being processed\n",
    "counter = 0\n",
    "# Find the total number of files that will be run by this cell \n",
    "for file in os.listdir(pdb_dir):\n",
    "    if file.endswith(\".cleaned.pdb\") and \"%s.5.4\"%file not in os.listdir(cor_dir) \\\n",
    "    and \"%s.MC.5.4\"%file not in os.listdir(cor_dir):\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "exceptions = []\n",
    "        \n",
    "run_counter = 0\n",
    "for file in os.listdir(pdb_dir):\n",
    "    if file.endswith(\".cleaned.pdb\") and \"%s.5.4\"%file not in os.listdir(i_am_here) \\\n",
    "    and \"%s.MC.5.4\"%file not in os.listdir(i_am_here):\n",
    "        \n",
    "        pdb_name = file.replace(\".cleaned.pdb\", \"\")\n",
    "\n",
    "        # Record the name of the file in byte form to input into COREX\n",
    "        bytename = bytes('%s/%s'%(pdb_dir, file), 'utf-8')\n",
    "\n",
    "        # Anticipate file names to avoid duplicate runs\n",
    "        reg_file = file + \".5.4\"\n",
    "        MC_file = file + \".MC.5.4\"\n",
    "        print(file)\n",
    "        print(reg_file)\n",
    "\n",
    "        file_name = (os.path.join(pdb_dir, file))\n",
    "        fo = open(file_name, \"r\")\n",
    "        lines = fo.readlines()\n",
    "        size = len(lines)\n",
    "        \n",
    "        print(file[0:4], \"has \", size, \"atoms\")\n",
    "\n",
    "        if size < 550 and reg_file not in os.listdir(cor_dir):\n",
    "            try:\n",
    "                # Run EnsembleGenerator for each PDB, then run a.out for each entropy scaling factor\n",
    "                subprocess.run('./COREX_Files/EnsembleGenerator', input=(bytename), shell=False, check=True)\n",
    "                \n",
    "                shutil.move('%s/%s.cleaned.pdb.5.4'%(pdb_dir, pdb_name), '%s/%s.cleaned.pdb.5.4'%(i_am_here, pdb_name))\n",
    "                shutil.move('%s/%s.cleaned.pdb.info'%(pdb_dir, pdb_name), '%s/%s.cleaned.pdb.info'%(i_am_here, pdb_name))\n",
    "\n",
    "                run_counter += 1\n",
    "                print()\n",
    "                print(\"You have run %s of %s files so far\"%(run_counter, counter))\n",
    "                print()\n",
    "\n",
    "            \n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                exceptions.append(file[0:4])\n",
    "\n",
    "        if size > 550 and MC_file not in os.listdir(cor_dir):\n",
    "            try:\n",
    "                # Run EnsembleGenerator for each PDB, then run a.out for each entropy scaling factor\n",
    "                subprocess.run('./COREX_Files/EnsembleGeneratorMC', input=(bytename), shell=False, check=True)\n",
    "                \n",
    "                shutil.move('%s/%s.cleaned.pdb.MC.5.4'%(pdb_dir, pdb_name), '%s/%s.cleaned.pdb.MC.5.4'%(i_am_here, pdb_name))\n",
    "                shutil.move('%s/%s.cleaned.pdb.info'%(pdb_dir, pdb_name), '%s/%s.cleaned.pdb.info'%(i_am_here, pdb_name))\n",
    "\n",
    "                run_counter += 1\n",
    "                print()\n",
    "                print(\"You have run %s of %s files so far\"%(run_counter, counter))\n",
    "                print()\n",
    "                \n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                exceptions.append(file[0:4])\n",
    "        \n",
    "print(exceptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATE THERMODESCIPT FILES\n",
    "\"\"\"\n",
    "# Calls the relavent a.out program to generate thermodescript files \n",
    "\n",
    "problemos = []\n",
    "\n",
    "# Iterate through directory and only run this section on files not processed\n",
    "for file in os.listdir(i_am_here):\n",
    "    if file.endswith(\".5.4\"):\n",
    "\n",
    "        check = file.replace(\".5.4\", \"\")\n",
    "\n",
    "        if \"%s.W5.T25S0.500.ThermoDescriptN\"%check not in os.listdir(i_am_here):\n",
    "\n",
    "            # Identify which are MC files and which are not\n",
    "            # Generate the native and denatured ThermoDescript files\n",
    "\n",
    "            try:\n",
    "            \n",
    "                if 'MC' in file:\n",
    "                    name = file.replace(\".MC.5.4\", \"\")\n",
    "                    byte_command = bytes(\"%s\\n\\n\\n\\n\\n\"%name, 'utf-8')\n",
    "\n",
    "                    native_program = \"./COREX_Files/a.out_nativeMC\"\n",
    "                    denatured_program = \"./COREX_Files/a.out_denaturedMC\"\n",
    "\n",
    "                    ps = subprocess.Popen(native_program , shell=False, stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n",
    "                    ps.communicate(input = byte_command)[0]\n",
    "\n",
    "                    ps = subprocess.Popen(denatured_program , shell=False, stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n",
    "                    ps.communicate(input = byte_command)[0]\n",
    "\n",
    "            \n",
    "                if not 'MC' in file:\n",
    "                    name = file.replace(\".5.4\", \"\")\n",
    "                    byte_command = bytes(\"%s\\n\\n\\n\\n\\n\"%name, 'utf-8')\n",
    "\n",
    "                    native_program = \"./COREX_Files/a.out_native\"\n",
    "                    denatured_program = \"./COREX_Files/a.out_denatured\"\n",
    "\n",
    "                    ps = subprocess.Popen(native_program , shell=False, stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n",
    "                    ps.communicate(input = byte_command)[0]\n",
    "\n",
    "                    ps = subprocess.Popen(denatured_program , shell=False, stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n",
    "                    ps.communicate(input = byte_command)[0]\n",
    "\n",
    "            except:\n",
    "                print(\"%s encountered an error and no ThermoDescript file was created\" %check)\n",
    "                problemos.append(check[0:4])\n",
    "\n",
    "# Move all of the files generated into the COREX Output directory\n",
    "for file in os.listdir(i_am_here):\n",
    "    if file.endswith(\".5.4\") or file.endswith('.info') or file.endswith('DescriptN') or file.endswith('DescriptD'):\n",
    "        shutil.move('%s/%s'%(i_am_here, file), '%s/%s'%(cor_dir, file))                \n",
    "                \n",
    "print(problemos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FIND THE LOG ODDS SCORE FOR THE NATIVE STARTING PROTEIN STRUCTURES\n",
    "\"\"\"\n",
    "\n",
    "scores = []\n",
    "\n",
    "for file in os.listdir(cor_dir):\n",
    "    if file.endswith(\"0.500.ThermoDescriptN\"):\n",
    "        other_file = file.replace(\"0.500.ThermoDescriptN\", \"1.500.ThermoDescriptD\")\n",
    "        \n",
    "        name = file[0:4]\n",
    "\n",
    "        n_name = (os.path.join(cor_dir, file))\n",
    "        d_name = (os.path.join(cor_dir, other_file))\n",
    "\n",
    "        native_df = pd.read_csv(n_name, delim_whitespace=True)\n",
    "        denat_df = pd.read_csv(d_name, delim_whitespace=True)\n",
    "\n",
    "        native_df.drop(native_df[native_df['Res'] == \"0\"].index, inplace = True)\n",
    "        denat_df.drop(denat_df[denat_df['Res'] == \"0\"].index, inplace = True)\n",
    "\n",
    "        native_lo = native_df['Odds'].tolist()\n",
    "        denat_lo = denat_df['Odds'].tolist()\n",
    "\n",
    "        native_stability = native_df['ln(k_f)'].tolist()\n",
    "        denat_stability = denat_df['ln(k_f)'].tolist()\n",
    "\n",
    "        native_cluster = native_df['NTE'].tolist()\n",
    "        denat_cluster = denat_df['DTE'].tolist()\n",
    "\n",
    "        protein_len = len(native_lo)\n",
    "\n",
    "        native_raw_score = sum(native_lo)\n",
    "        denat_raw_score = sum(denat_lo)\n",
    "\n",
    "        # Establish mu and sigma based off of equations shared by Jamie and published by Hoffmann 2016\n",
    "        denat_mu = 0.105257632276167 - (0.173352418832684 * protein_len)\n",
    "        denat_sigma = 0.609622745362049 * math.sqrt(protein_len)\n",
    "\n",
    "        nat_mu = -0.0147176244982114 - (0.133774947797257 * protein_len)\n",
    "        nat_sigma = 0.540008754110101 * math.sqrt(protein_len)\n",
    "\n",
    "\n",
    "        n_cdf = scipy.stats.norm.cdf(native_raw_score, nat_mu, nat_sigma)\n",
    "        d_cdf = scipy.stats.norm.cdf(denat_raw_score, denat_mu, denat_sigma)\n",
    "\n",
    "        try:\n",
    "            # Calculate -logP values \n",
    "            native_logp = -(math.log(1 - (n_cdf), 10))\n",
    "            denat_logp = -(math.log(1 - (d_cdf), 10))\n",
    "\n",
    "            total_logp = native_logp + denat_logp\n",
    "\n",
    "            scores.append([name, native_logp, denat_logp, total_logp])\n",
    "            \n",
    "            df = pd.DataFrame(scores, columns = ['PDB', 'Native LogP', 'Denatured LogP', 'Total LogP'])\n",
    "            df.to_csv('%s_corex_data.csv'%date)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
